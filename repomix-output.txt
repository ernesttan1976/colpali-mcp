This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-05-13T06:13:13.212Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
.env.example
.gitattributes
.gitignore
app_patch.py
app.py
db.py
docker-compose.yml
docker-entrypoint.sh
Dockerfile
model_setup.sh
packages.txt
README.md
start-app.sh
verify_cuda.py

================================================================
Repository Files
================================================================

================
File: .env.example
================
OPENAI_API_KEY=sk-<api-key-here>

================
File: .gitattributes
================
*.7z filter=lfs diff=lfs merge=lfs -text
*.arrow filter=lfs diff=lfs merge=lfs -text
*.bin filter=lfs diff=lfs merge=lfs -text
*.bz2 filter=lfs diff=lfs merge=lfs -text
*.ckpt filter=lfs diff=lfs merge=lfs -text
*.ftz filter=lfs diff=lfs merge=lfs -text
*.gz filter=lfs diff=lfs merge=lfs -text
*.h5 filter=lfs diff=lfs merge=lfs -text
*.joblib filter=lfs diff=lfs merge=lfs -text
*.lfs.* filter=lfs diff=lfs merge=lfs -text
*.mlmodel filter=lfs diff=lfs merge=lfs -text
*.model filter=lfs diff=lfs merge=lfs -text
*.msgpack filter=lfs diff=lfs merge=lfs -text
*.npy filter=lfs diff=lfs merge=lfs -text
*.npz filter=lfs diff=lfs merge=lfs -text
*.onnx filter=lfs diff=lfs merge=lfs -text
*.ot filter=lfs diff=lfs merge=lfs -text
*.parquet filter=lfs diff=lfs merge=lfs -text
*.pb filter=lfs diff=lfs merge=lfs -text
*.pickle filter=lfs diff=lfs merge=lfs -text
*.pkl filter=lfs diff=lfs merge=lfs -text
*.pt filter=lfs diff=lfs merge=lfs -text
*.pth filter=lfs diff=lfs merge=lfs -text
*.rar filter=lfs diff=lfs merge=lfs -text
*.safetensors filter=lfs diff=lfs merge=lfs -text
saved_model/**/* filter=lfs diff=lfs merge=lfs -text
*.tar.* filter=lfs diff=lfs merge=lfs -text
*.tar filter=lfs diff=lfs merge=lfs -text
*.tflite filter=lfs diff=lfs merge=lfs -text
*.tgz filter=lfs diff=lfs merge=lfs -text
*.wasm filter=lfs diff=lfs merge=lfs -text
*.xz filter=lfs diff=lfs merge=lfs -text
*.zip filter=lfs diff=lfs merge=lfs -text
*.zst filter=lfs diff=lfs merge=lfs -text
*tfevents* filter=lfs diff=lfs merge=lfs -text
climate_youth_magazine.pdf filter=lfs diff=lfs merge=lfs -text

================
File: .gitignore
================
# Virtual Environment
venv/
.venv/
env/
ENV/

# Python cache files
__pycache__/
*.py[cod]
*$py.class
*.so
.Python

# Distribution / packaging
dist/
build/
*.egg-info/

# Local development settings
.env
.dockerignore

# IDE specific files
.idea/
.vscode/
*.swp
*.swo

models/
data/embeddings_db/

================
File: app_patch.py
================
#!/usr/bin/env python3
"""
Patch for modifying app.py to disable flash-attention installation attempt.
Run this before starting the application if you want to disable the flash-attention install.
"""

import os
import re
import shutil

# Path to the app.py file
APP_FILE = 'app.py'

# Make a backup of the original file
backup_file = f"{APP_FILE}.backup"
if not os.path.exists(backup_file):
    shutil.copy2(APP_FILE, backup_file)
    print(f"Created backup file: {backup_file}")

# Read the app.py file
with open(APP_FILE, 'r') as f:
    content = f.read()

# Check if the install_fa2 function is being called
if '@spaces.GPU\ndef install_fa2():' in content and 'install_fa2()' in content:
    # Comment out the call to install_fa2 function
    modified_content = re.sub(
        r'(# )?install_fa2\(\)',
        '# install_fa2()  # Disabled for Docker',
        content
    )

    # Write the modified content back to app.py
    with open(APP_FILE, 'w') as f:
        f.write(modified_content)
    
    print(f"Modified {APP_FILE} to disable flash-attention installation")
else:
    print(f"No modifications needed for {APP_FILE}")

================
File: app.py
================
import os
import spaces
import base64
from io import BytesIO
import io
import requests

import gradio as gr
import torch

from pdf2image import convert_from_path, convert_from_bytes
from PIL import Image
from torch.utils.data import DataLoader
from tqdm import tqdm

from colpali_engine.models import ColQwen2, ColQwen2Processor

import sys

from db import DocumentEmbeddingDatabase

import zipfile
from datetime import datetime

# Create the directory for the embeddings database if it doesn't exist
os.makedirs("./data/embeddings_db", exist_ok=True)

# Initialize the database - this connects to the local file-based database
db = DocumentEmbeddingDatabase(db_path="./data/embeddings_db")

print(f"Python path: {sys.executable}")
print(f"PyTorch version: {torch.__version__}")
print(f"PyTorch path: {torch.__file__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")

# Define model paths
MODEL_DIR = "./models/colqwen2"
MODEL_PATH = os.path.join(MODEL_DIR, "model")
PROCESSOR_PATH = os.path.join(MODEL_DIR, "processor")
MODEL_MARKER = os.path.join(MODEL_DIR, "model_loaded.marker")

PAGE_LIMIT = 800

# LLM Provider options for the dropdown
LLM_PROVIDERS = [
    "OpenAI GPT-4o-mini",
    "Anthropic Claude 3.7 Sonnet"
]

def verify_model_directories():
    """Verify that model directories exist and are writable."""
    print("=== Verifying Model Directories ===")
    
    # Ensure directories exist
    os.makedirs(MODEL_PATH, exist_ok=True)
    os.makedirs(PROCESSOR_PATH, exist_ok=True)
    
    print(f"MODEL_DIR: {MODEL_DIR} - Exists: {os.path.exists(MODEL_DIR)}")
    print(f"MODEL_PATH: {MODEL_PATH} - Exists: {os.path.exists(MODEL_PATH)}")
    print(f"PROCESSOR_PATH: {PROCESSOR_PATH} - Exists: {os.path.exists(PROCESSOR_PATH)}")
    
    # Verify we can write to these directories
    try:
        test_file_model = os.path.join(MODEL_PATH, 'test_write.tmp')
        test_file_processor = os.path.join(PROCESSOR_PATH, 'test_write.tmp')
        
        with open(test_file_model, 'w') as f:
            f.write('test')
        with open(test_file_processor, 'w') as f:
            f.write('test')
            
        # Clean up
        os.remove(test_file_model)
        os.remove(test_file_processor)
        print("âœ… Model directories are writable - volume mounting is working correctly")
        return True
    except Exception as e:
        print(f"âŒ Error: Cannot write to model directories. Docker volume may not be mounted correctly: {e}")
        print("Please ensure that './models' directory exists and has proper permissions")
        return False

def check_model_persistence():
    """Check if model persistence marker exists from previous runs."""
    if os.path.exists(MODEL_MARKER):
        with open(MODEL_MARKER, 'r') as f:
            marker_content = f.read()
            print(f"âœ… Model persistence confirmed! Previous marker: {marker_content}")
        return True
    print("No model persistence marker found - this might be the first run")
    return False

def mark_model_loaded():
    """Create a marker file indicating the model has been loaded successfully."""
    try:
        with open(MODEL_MARKER, 'w') as f:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"Model loaded successfully at {timestamp}")
        print(f"Created model marker file at {MODEL_MARKER}")
    except Exception as e:
        print(f"Warning: Could not create model marker file: {e}")


def create_zip_for_download(query, response, images):
    """
    Create a zip file containing the query, response, and retrieved images.
    
    Args:
        query (str): The user's query
        response (str): The AI's response to the query
        images (list): List of (image, caption) tuples from the search results
        
    Returns:
        bytes: The zip file as bytes
    """
    # Create an in-memory zip file
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        # Create markdown file with query and response
        markdown_content = f"# Query\n\n{query}\n\n# AI Response\n\n{response}"
        zip_file.writestr("query_response.md", markdown_content)
        
        # Add images to zip file
        for i, (image, caption) in enumerate(images):
            # Save image to bytes
            img_buffer = io.BytesIO()
            image.save(img_buffer, format="JPEG")
            img_buffer.seek(0)
            
            # Add to zip with caption as part of filename
            clean_caption = caption.replace('/', '_').replace('\\', '_')
            zip_file.writestr(f"image_{i+1}_{clean_caption}.jpg", img_buffer.getvalue())
    
    # Return the zip file as bytes
    zip_buffer.seek(0)
    return zip_buffer.getvalue()


@spaces.GPU
def install_fa2():
    print("Install FA2")
    os.system("pip install flash-attn --no-build-isolation")
# install_fa2()  # Disabled for Docker

def load_model():
    """Load model from disk if available, otherwise download and save it."""
    try:
        # Verify directories are properly set up before proceeding
        if not verify_model_directories():
            print("WARNING: Model directories verification failed, but will try to continue")
        
        # Check if model was previously loaded successfully
        model_persistence = check_model_persistence()
        
        os.makedirs(MODEL_DIR, exist_ok=True)
        print(f"Model directory: {MODEL_DIR}")
        print(f"Model directory exists: {os.path.exists(MODEL_DIR)}")
        
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {device}")
        print(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDA device count: {torch.cuda.device_count()}")
            print(f"CUDA device name: {torch.cuda.get_device_name(0)}")
        
        # Check if critical model files exist - any of these combinations are valid
        model_files_exist = (
            (os.path.exists(os.path.join(MODEL_PATH, "config.json"))) or
            (os.path.exists(os.path.join(MODEL_PATH, "adapter_config.json")) and 
             os.path.exists(os.path.join(MODEL_PATH, "adapter_model.safetensors")) and
             os.path.exists(os.path.join(MODEL_PATH, "generation_config.json")))
        )
        
        processor_files_exist = (
            (os.path.exists(os.path.join(PROCESSOR_PATH, "config.json"))) or
            (os.path.exists(os.path.join(PROCESSOR_PATH, "tokenizer_config.json")) and
             os.path.exists(os.path.join(PROCESSOR_PATH, "tokenizer.json")) and
             os.path.exists(os.path.join(PROCESSOR_PATH, "vocab.json")))
        )
        
        print(f"Model files exist: {model_files_exist}")
        print(f"Processor files exist: {processor_files_exist}")
        
        # List files in model directory to debug
        if os.path.exists(MODEL_PATH):
            print(f"Files in model directory: {os.listdir(MODEL_PATH)}")
        if os.path.exists(PROCESSOR_PATH):
            print(f"Files in processor directory: {os.listdir(PROCESSOR_PATH)}")
        
        # Only attempt to load if critical files exist
        if model_files_exist and processor_files_exist:
            print("Loading model from disk - step 1...")
            try:
                # Use absolute paths to avoid any reference issues
                abs_model_path = os.path.abspath(MODEL_PATH)
                abs_processor_path = os.path.abspath(PROCESSOR_PATH)
                print(f"Using absolute model path: {abs_model_path}")
                
                # Load model with trust_remote_code
                print("Loading model from disk - step 2...")
                model = ColQwen2.from_pretrained(
                    abs_model_path,
                    torch_dtype=torch.bfloat16,
                    device_map=device,
                    local_files_only=True,  # Changed to True to force local loading
                    trust_remote_code=True,
                    revision=None  # Important: don't try to fetch remote info
                )
                print("Model loaded successfully!")
                
                print("Loading processor...")
                processor = ColQwen2Processor.from_pretrained(
                    abs_processor_path,
                    local_files_only=True,
                    trust_remote_code=True,
                    revision=None
                )
                print("Processor loaded successfully!")
                
                print("Putting model in evaluation mode...")
                model = model.eval()
                print("Model ready!")
                
                # Mark that model was loaded successfully
                mark_model_loaded()
                
                return model, processor
            except Exception as e:
                print(f"Error loading model from disk: {e}")
                print(f"Error type: {type(e)}")
                print("Forcing download of a new model...")
                # Force download new model
                return download_model(device)
        else:
            print("Model files not found or incomplete on disk, downloading...")
            return download_model(device)
    except Exception as e:
        print(f"Exception in load_model: {e}")
        print(f"Exception type: {type(e)}")
        raise

def download_model(device):
    print("Downloading model (first run only)...")
    try:
        print("Download step 1 - initializing...")
        model = ColQwen2.from_pretrained(
            "vidore/colqwen2-v1.0",
            torch_dtype=torch.bfloat16,
            device_map=device,
            trust_remote_code=True
        )
        print("Download step 2 - model downloaded successfully!")
        
        print("Setting model to eval mode...")
        model = model.eval()
        
        print("Downloading processor...")
        processor = ColQwen2Processor.from_pretrained(
            "vidore/colqwen2-v1.0",
            trust_remote_code=True
        )
        print("Processor downloaded successfully!")
        
        # Save model and processor to disk with absolute paths
        print("Saving model to disk for future use...")
        try:
            abs_model_path = os.path.abspath(MODEL_PATH)
            abs_processor_path = os.path.abspath(PROCESSOR_PATH)
            
            print(f"Saving model to {abs_model_path}...")
            model.save_pretrained(abs_model_path)
            print(f"Model saved successfully!")
            
            print(f"Saving processor to {abs_processor_path}...")
            processor.save_pretrained(abs_processor_path)
            print(f"Processor saved successfully!")
            
            # Mark that model was loaded and saved successfully
            mark_model_loaded()
            
        except Exception as e:
            print(f"Error saving model to disk: {e}")
            print(f"Error type: {type(e)}")
    except Exception as e:
        print(f"Error downloading model: {e}")
        print(f"Error type: {type(e)}")
        raise
    
    return model, processor

# Verify model directories and persistence before loading
verify_model_directories()
check_model_persistence()

# Load model and processor
model, processor = load_model()


def encode_image_to_base64(image):
    """Encodes a PIL image to a base64 string."""
    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")


def query_claude(query, images, api_key):
    """Calls Anthropic's Claude 3.7 Sonnet with the query and image data."""

    if not api_key or not api_key.startswith("sk-ant"):
        return "Enter your Anthropic API key to get a response from Claude 3.7 Sonnet"
    
    try:
        # Format the Claude prompt
        CLAUDE_PROMPT = """
        You are a smart assistant designed to answer questions about a PDF document.
        You are given relevant information in the form of PDF pages. Use them to construct a detailed response to the question, and cite your sources (page numbers, etc).
        If it is not possible to answer using the provided pages, do not attempt to provide an answer and simply say the answer is not present within the documents.
        Give detailed and extensive answers, only containing info in the pages you are given.
        You can answer using information contained in plots and figures if necessary.
        Answer in the same language as the query.
        
        Query: {query}
        PDF pages:
        """
        
        # Create the Anthropic API request
        url = "https://api.anthropic.com/v1/messages"
        headers = {
            "x-api-key": api_key.strip(),
            "anthropic-version": "2023-06-01",
            "content-type": "application/json"
        }
        
        # Prepare the message content with text and images
        content = [
            {"type": "text", "text": CLAUDE_PROMPT.format(query=query)}
        ]
        
        # Add images to the content
        for i, (image, caption) in enumerate(images):
            buffered = BytesIO()
            image.save(buffered, format="JPEG")
            img_bytes = buffered.getvalue()
            img_base64 = base64.b64encode(img_bytes).decode("utf-8")
            
            content.append({
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": img_base64
                }
            })
            
            # Add caption as text after each image
            content.append({
                "type": "text",
                "text": f"\n{caption}\n"
            })
        
        # Construct the final request payload
        data = {
            "model": "claude-3-7-sonnet-20250219",
            "max_tokens": 4000,
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ]
        }
        
        # Send the request to Anthropic
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        result = response.json()
        
        return result["content"][0]["text"]
    
    except Exception as e:
        return f"Anthropic API connection failure. Error: {e}"


def query_gpt4o_mini(query, images, api_key):
    """Calls OpenAI's GPT-4o-mini with the query and image data."""

    if not api_key or not api_key.startswith("sk-"):
        return "Enter your OpenAI API key to get a response from GPT-4o-mini"
    
    try:
        from openai import OpenAI
    
        base64_images = [encode_image_to_base64(image[0]) for image in images]
        client = OpenAI(api_key=api_key.strip())
        PROMPT = """
        You are a smart assistant designed to answer questions about a PDF document.
        You are given relevant information in the form of PDF pages. Use them to construct a detailed response to the question, and cite your sources (page numbers, etc).
        If it is not possible to answer using the provided pages, do not attempt to provide an answer and simply say the answer is not present within the documents.
        Give detailed and extensive answers, only containing info in the pages you are given.
        You can answer using information contained in plots and figures if necessary.
        Answer in the same language as the query.
        
        Query: {query}
        PDF pages:
        """
    
        response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
              "role": "user",
              "content": [
                {
                  "type": "text",
                  "text": PROMPT.format(query=query)
                }] + [{
                  "type": "image_url",
                  "image_url": {
                    "url": f"data:image/jpeg;base64,{im}"
                    },
                } for im in base64_images]
            }
          ],
          max_tokens=8000,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"OpenAI API connection failure. Error: {e}"


def parse_api_keys(api_key_input):
    """
    Parse API key input that might contain multiple keys in format:
    openai:sk-xxx|anthropic:sk-ant-xxx or just a single key
    
    Returns a dictionary of provider:key pairs
    """
    api_keys = {}
    
    if '|' in api_key_input:
        for key_pair in api_key_input.split('|'):
            if ':' in key_pair:
                provider, key = key_pair.split(':', 1)
                api_keys[provider.strip().lower()] = key.strip()
    else:
        # Try to determine the provider based on key format
        key = api_key_input.strip()
        if key.startswith('sk-ant'):
            api_keys['anthropic'] = key
        elif key.startswith('sk-'):
            api_keys['openai'] = key
    
    return api_keys


def query_llm(query, images, api_key_input, llm_provider):
    """
    Route the query to the appropriate LLM based on the selected provider.
    
    Args:
        query (str): The user's query
        images (list): List of (image, caption) tuples from the search results
        api_key_input (str): The API key for the selected provider
        llm_provider (str): The provider/model to use
        
    Returns:
        str: The LLM's response
    """
    # Parse API keys
    api_keys = parse_api_keys(api_key_input)
    
    # Handle the routing based on provider
    if llm_provider == "OpenAI GPT-4o-mini":
        # Check if we have a specific OpenAI key
        openai_key = api_keys.get('openai', api_key_input)
        return query_gpt4o_mini(query, images, openai_key)
        
    elif llm_provider == "Anthropic Claude 3.7 Sonnet":
        # Check if we have a specific Anthropic key
        anthropic_key = api_keys.get('anthropic', api_key_input)
        return query_claude(query, images, anthropic_key)
        
    else:
        return f"Unknown LLM provider: {llm_provider}. Please select a valid option."


# Modify the search function to correctly format the return value for Gradio File component
@spaces.GPU
def search(query: str, ds, images, k, api_key, llm_provider):
    try:
        k = min(k, len(ds))
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        if device != model.device:
            model.to(device)
            
        qs = []
        with torch.no_grad():
            batch_query = processor.process_queries([query]).to(model.device)
            embeddings_query = model(**batch_query)
            qs.extend(list(torch.unbind(embeddings_query.to("cpu"))))

        scores = processor.score(qs, ds, device=device)

        top_k_indices = scores[0].topk(k).indices.tolist()

        results = []
        for idx in top_k_indices:
            results.append((images[idx], f"Page {idx}"))

        # Generate response from the selected LLM
        ai_response = query_llm(query, results, api_key, llm_provider)

        # Create download data
        download_data = None
        if results and ai_response:
            try:
                # Create timestamp for download filename
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"colpali_results_{timestamp}.zip"
                
                # Create the temporary file path where we'll save the zip
                import tempfile
                temp_dir = os.path.join(tempfile.gettempdir(), "colpali_downloads")
                os.makedirs(temp_dir, exist_ok=True)
                temp_zip_path = os.path.join(temp_dir, filename)
                
                # Create zip file content
                zip_data = create_zip_for_download(query, ai_response, results)
                
                # Write to temp file
                with open(temp_zip_path, "wb") as f:
                    f.write(zip_data)
                
                # Return the file path for Gradio File component
                download_data = temp_zip_path
            except Exception as e:
                print(f"Error creating download file: {e}")
                import traceback
                traceback.print_exc()
        
        return results, ai_response, download_data
    except Exception as e:
        import traceback
        error_msg = f"Error in search function: {str(e)}\n{traceback.format_exc()}"
        print(error_msg)
        return [], error_msg, None

def convert_files(files):
    """Convert uploaded files to images, handling different file types from Gradio."""
    images = []
    
    for f in files:
        try:
            # Handle the file based on its type
            if hasattr(f, 'name'):
                # This is likely a file object with a name attribute
                file_path = f.name
                print(f"Processing file with path: {file_path}")
                images.extend(convert_from_path(file_path, thread_count=4))
            elif isinstance(f, tuple) and len(f) == 2:
                # If it's a tuple of (name, file-like object) as returned by some Gradio versions
                temp_name, temp_file = f
                print(f"Processing tuple with name: {temp_name}")
                # If it's a file-like object, read it and convert from bytes
                if hasattr(temp_file, 'read'):
                    file_content = temp_file.read()
                    images.extend(convert_from_bytes(file_content, thread_count=4))
                else:
                    # If it's a path
                    images.extend(convert_from_path(temp_file, thread_count=4))
            elif isinstance(f, str):
                # If it's directly a file path
                print(f"Processing file path: {f}")
                images.extend(convert_from_path(f, thread_count=4))
            else:
                # Try to get the file path from the object
                print(f"Unknown file type: {type(f)}, trying to handle generically")
                if hasattr(f, 'file'):
                    # Some Gradio versions provide a file attribute
                    file_content = f.file.read()
                    images.extend(convert_from_bytes(file_content, thread_count=4))
                elif hasattr(f, 'read'):
                    # If it's a file-like object
                    file_content = f.read()
                    images.extend(convert_from_bytes(file_content, thread_count=4))
                else:
                    raise TypeError(f"Unsupported file type: {type(f)}. Please provide a valid PDF file.")
        except Exception as e:
            print(f"Error processing file {f}: {e}")
            import traceback
            traceback.print_exc()
            # Continue with other files rather than failing completely
            continue

    if len(images) > PAGE_LIMIT:
        raise gr.Error(f"The number of images in the dataset should be less than {PAGE_LIMIT}.")
    
    if not images:
        raise ValueError("No valid PDF files were processed. Please check your uploads.")
        
    return images


def index(files, ds):
    try:
        print("Converting files")
        print(f"File types: {[type(f) for f in files]}")
        
        # Reset the embeddings list and images list
        ds = []
        all_images = []
        
        for f in files:
            # Get the file path and ensure we have a valid filename
            if hasattr(f, 'name'):
                file_path = f.name
            elif isinstance(f, tuple) and len(f) == 2:
                file_path = f[0]  # Use the name from the tuple
            elif isinstance(f, str):
                file_path = f
            else:
                # Try other approaches to get the path
                if hasattr(f, 'file'):
                    file_path = str(f.file)
                else:
                    # Generate a temporary unique identifier if we can't get the path
                    import hashlib
                    file_path = f"unknown_file_{hashlib.md5(str(f).encode()).hexdigest()}"
            
            filename = os.path.basename(file_path)
            print(f"Processing file: {filename} (path: {file_path})")
            
            # Check if embeddings exist for this file - THIS USES ONLY THE FILENAME NOW
            if db.embeddings_exist(filename):  # <-- Change here: using filename instead of file_path
                print(f"Loading existing embeddings for {filename}")
                
                # Convert file to images for display only
                try:
                    images = convert_files([f])
                    if not images:
                        print(f"Could not convert {filename} to images")
                        continue
                except Exception as e:
                    print(f"Error converting file to images: {e}")
                    continue
                
                # Load embeddings from database using the filename
                file_embeddings = db.load_embeddings(filename)  # <-- Change here: using filename instead of file_path
                
                if file_embeddings and len(file_embeddings) > 0:
                    # Add to our lists
                    ds.extend(file_embeddings)
                    all_images.extend(images)
                    
                    print(f"Loaded {len(file_embeddings)} existing embeddings")
                else:
                    print(f"Failed to load embeddings, will regenerate")
                    # Fall back to generating new embeddings
                    process_new_file(f, filename, ds, all_images)  # <-- Change here: using filename
            else:
                print(f"No existing embeddings for {filename}, generating new ones")
                # Process the file normally
                process_new_file(f, filename, ds, all_images)  # <-- Change here: using filename
                
        return f"Processed {len(files)} files with {len(ds)} total embeddings", ds, all_images
    except Exception as e:
        import traceback
        traceback_str = traceback.format_exc()
        return f"Error in indexing: {str(e)}\n{traceback_str}", ds, []


def process_new_file(f, file_id, ds, all_images):
    """Process a file by generating new embeddings and saving them"""
    try:
        # Convert file to images
        images = convert_files([f])
        
        if not images:
            print(f"Could not convert {file_id} to images")
            return
            
        # Get embeddings for this file only
        file_ds = []
        status, file_ds, _ = index_gpu(images, file_ds)
        
        # Save the new embeddings if successful
        if file_ds and len(file_ds) > 0:
            saved = db.save_embeddings(file_id, file_ds, len(images))
            if saved:
                print(f"Saved {len(file_ds)} new embeddings for {file_id}")
            else:
                print(f"Failed to save embeddings for {file_id}")
            
            # Add to our complete lists
            ds.extend(file_ds)
            all_images.extend(images)
        else:
            print(f"Failed to generate embeddings for {file_id}")
    except Exception as e:
        print(f"Error processing new file {file_id}: {e}")
        import traceback
        traceback.print_exc()
        
# Modified index_gpu function (keeping the core functionality the same)
@spaces.GPU
def index_gpu(images, ds):
    """Example script to run inference with ColPali (ColQwen2)"""
    try:
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        if device != model.device:
            model.to(device)
            
        # run inference - docs
        dataloader = DataLoader(
            images,
            batch_size=1,
            # num_workers=2,
            shuffle=False,
            collate_fn=lambda x: processor.process_images(x).to(model.device),
        )

        for batch_doc in tqdm(dataloader):
            with torch.no_grad():
                batch_doc = {k: v.to(device) for k, v in batch_doc.items()}
                embeddings_doc = model(**batch_doc)
            ds.extend(list(torch.unbind(embeddings_doc.to("cpu"))))
        return f"Uploaded and converted {len(images)} pages", ds, images
    except Exception as e:
        import traceback
        traceback_str = traceback.format_exc()
        return f"Error in processing: {str(e)}\n{traceback_str}", ds, []

# Update the Gradio UI section for better file handling
with gr.Blocks(theme=gr.themes.Glass(),title="RTFM") as demo:
    gr.Markdown("# Â®ï¸etrieval For Technical â“‚ï¸anuals (RTFM) ðŸ˜œ")
    with gr.Accordion("Details:", open=False):
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown(""" ## Problem: ## 
                            Typical RAG fails for highly graphical documents.""")
            with gr.Column(scale=1):
                gr.Markdown(""" ## Solution ##
                    The ColPali model's results are promising! 
                    1. ColPali's strategy is to ingest each pdf page as an image. 
                    2. These embeddings are stored in a LanceDB embeddings database for persistence.
                    3. The query is first sent to the Colpali model, returning responses in the form of images with page number.
                    4. This is then passed on to your selected AI (OpenAI GPT-4o-mini or Anthropic Claude 3.7) to get the final response.
                    """)
    
    with gr.Row():
        with gr.Column(scale=2):
            gr.Markdown("## 1ï¸âƒ£ Upload PDFs")
            file = gr.File(file_types=["pdf"], file_count="multiple", label="Upload PDFs")

            convert_button = gr.Button("ðŸ”„ Index documents")
            message = gr.Textbox("Files not yet uploaded", label="Status")
            
            # Create a dropdown for the LLM provider
            llm_provider = gr.Dropdown(
                choices=LLM_PROVIDERS,
                label="Select AI Model for Response Generation",
                value=LLM_PROVIDERS[0],
                info="Choose which AI model will answer your questions using the retrieved PDF pages."
            )
            
            # Add API key input with improved description
            api_key = gr.Textbox(
                placeholder="Enter API key(s): openai:sk-xxx|anthropic:sk-ant-xxx or just paste single key",
                label="API key",
                value=os.getenv('OPENAI_API_KEY','') if llm_provider==LLM_PROVIDERS[0] else os.getenv('ANTHROPIC_API_KEY', ''),
                type="password",
                info="Enter your OpenAI or Anthropic API key"
            )
            
            embeds = gr.State(value=[])
            imgs = gr.State(value=[])

        with gr.Column(scale=3):
            gr.Markdown("## 2ï¸âƒ£ Search")
            query = gr.Textbox(placeholder="Enter your query here", label="Query")
            k = gr.Slider(minimum=1, maximum=10, step=1, label="Number of results", value=5)
            search_button = gr.Button("ðŸ” Search", variant="primary")

    # Define the outputs first
    output_gallery = gr.Gallery(label="Retrieved Documents", height=800, show_label=True, show_share_button=True, columns=[5], rows=[1], object_fit="contain")
    output_text = gr.Textbox(label="AI Response", placeholder="Generated response based on retrieved documents", show_copy_button=True)
    download_file = gr.File(label="Download Results", visible=False)

    # Define the actions
    convert_button.click(index, inputs=[file, embeds], outputs=[message, embeds, imgs])
    
    # Update search button click to include the LLM provider
    search_result = search_button.click(
        search, 
        inputs=[query, embeds, imgs, k, api_key, llm_provider], 
        outputs=[output_gallery, output_text, download_file]
    )
    
    # Add event listener to update API key when LLM provider changes
    llm_provider.change(
        fn=lambda provider: os.getenv('OPENAI_API_KEY', '') if provider == LLM_PROVIDERS[0] else os.getenv('ANTHROPIC_API_KEY', ''),
        inputs=[llm_provider],
        outputs=[api_key]
    )
        
    # Show download file when search completes with results
    search_result.then(
        lambda file: gr.update(visible=file is not None),
        [download_file],
        download_file
    )

    with gr.Accordion("API Keys Usage:", open=False):
        gr.Markdown("""
        ## API Key Format
        - For OpenAI GPT-4o-mini: Enter your OpenAI API key starting with `sk-`
        - For Anthropic Claude 3.7: Enter your Anthropic API key starting with `sk-ant-`
        
        ## Combined Format (Optional)
        You can provide both API keys in the format: `openai:sk-xxx|anthropic:sk-ant-xxx`
        
        This allows you to switch between models without changing the API key each time.
        """)

    with gr.Accordion("Acknowledgements:", open=False):
        gr.Markdown("# ColPali: Efficient Document Retrieval with Vision Language Models (ColQwen2) ðŸ“š")
        gr.Markdown("""Demo to test ColQwen2 (ColPali) on PDF documents. 
        ColPali is model implemented from the [ColPali paper](https://arxiv.org/abs/2407.01449).

        This demo allows you to upload PDF files and search for the most relevant pages based on your query.
        Refresh the page if you change documents!

        âš ï¸ This demo uses a model trained exclusively on A4 PDFs in portrait mode, containing english text. Performance is expected to drop for other page formats and languages.
        Other models will be released with better robustness towards different languages and document formats!
        """)

if __name__ == "__main__":
    # Use a simpler launch method to avoid compatibility issues
    import atexit
    import shutil
    import tempfile
    
    # Create a temporary directory for file uploads if it doesn't exist
    temp_dir = os.path.join(tempfile.gettempdir(), "colpali_uploads")
    os.makedirs(temp_dir, exist_ok=True)
    
    # Clean up function to remove temp files
    def cleanup():
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    # Register the cleanup function
    atexit.register(cleanup)
    
    # Launch Gradio with simplified server settings
    demo.queue(max_size=10).launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    )

================
File: db.py
================
"""
Database module for storing and retrieving document embeddings using LanceDB.
"""

import os
import torch
import lancedb
import numpy as np
import pyarrow as pa
from pathlib import Path
from typing import List, Optional, Dict, Any, Union
import hashlib


class DocumentEmbeddingDatabase:
    """
    A class to handle storage and retrieval of document embeddings using LanceDB.
    """
    
    def __init__(self, db_path: str = "./data/embeddings_db"):
        """Initialize the database connection."""
        self.db_path = db_path
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
        self.db = lancedb.connect(db_path)
        print(f"Connected to LanceDB at {db_path}")
        
    def get_table_name_for_file(self, file_path: str) -> str:
        """
        Generate a consistent table name for a given file path.
        Only uses the filename, ignoring the path.
        """
        file_name = os.path.basename(file_path)
        file_name = Path(file_name).stem
        table_name = ''.join(c if c.isalnum() else '_' for c in file_name)
        return f"doc_{table_name}"
        
    def embeddings_exist(self, file_path: str) -> bool:
        """Check if embeddings for a file already exist in the database."""
        table_name = self.get_table_name_for_file(file_path)
        try:
            # Check if the table exists in LanceDB
            exists = table_name in self.db.table_names()
            
            # Also check the direct storage method
            doc_dir = os.path.join(self.db_path, "embeddings", table_name)
            exists_direct = os.path.exists(doc_dir)
            
            if exists or exists_direct:
                file_name = os.path.basename(file_path)
                print(f"Found existing embeddings for {file_name}")
                return True
            return False
        except Exception as e:
            print(f"Error checking if embeddings exist: {e}")
            return False
    
    def save_embeddings_direct(self, file_path: str, embeddings: List[torch.Tensor], page_count: int) -> bool:
        """Save embeddings using direct file writing to bypass LanceDB and PyArrow issues."""
        try:
            # Only use the filename for consistent table naming
            table_name = self.get_table_name_for_file(file_path)
            file_name = os.path.basename(file_path)
            
            print(f"Saving {len(embeddings)} embeddings for {file_name} using direct method")
            
            # Create a directory for this document
            doc_dir = os.path.join(self.db_path, "embeddings", table_name)
            os.makedirs(doc_dir, exist_ok=True)
            
            # Save metadata - store both filename and original path
            embedding_dim = embeddings[0].shape[0] if torch.is_tensor(embeddings[0]) else len(embeddings[0])
            metadata = {
                "filename": file_name,
                "original_path": file_path,
                "page_count": page_count,
                "embedding_dimension": embedding_dim,
                "dtype": str(embeddings[0].dtype) if torch.is_tensor(embeddings[0]) else "unknown"
            }
            
            import json
            with open(os.path.join(doc_dir, "metadata.json"), 'w') as f:
                json.dump(metadata, f)
                
            # Save each embedding as a separate file
            for i, embedding in enumerate(embeddings):
                if torch.is_tensor(embedding):
                    # Convert tensor to float32 numpy array
                    if embedding.dtype in [torch.bfloat16, torch.float16]:
                        embedding = embedding.to(torch.float32)
                    embedding_np = embedding.cpu().detach().numpy().astype(np.float32)
                else:
                    embedding_np = np.array(embedding, dtype=np.float32)
                
                # Save as numpy file
                np.save(os.path.join(doc_dir, f"embedding_{i:04d}.npy"), embedding_np)
            
            print(f"Successfully saved {len(embeddings)} embeddings for {file_name}")
            return True
        except Exception as e:
            print(f"Error in direct save: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def save_embeddings(self, file_path: str, embeddings: List[torch.Tensor], page_count: int) -> bool:
        """Save document embeddings to the database."""
        # Use direct file saving as a more reliable method
        return self.save_embeddings_direct(file_path, embeddings, page_count)
        
    def load_embeddings_direct(self, file_path: str) -> Optional[List[torch.Tensor]]:
        """Load embeddings using direct file reading."""
        try:
            table_name = self.get_table_name_for_file(file_path)
            file_name = os.path.basename(file_path)
            doc_dir = os.path.join(self.db_path, "embeddings", table_name)
            
            if not os.path.exists(doc_dir):
                return None
                
            print(f"Loading embeddings for {file_name} using direct method")
            
            # Load metadata
            import json
            try:
                with open(os.path.join(doc_dir, "metadata.json"), 'r') as f:
                    metadata = json.load(f)
                    print(f"Loaded metadata: {metadata['filename']} with {metadata.get('page_count', 0)} pages")
            except:
                metadata = {"dtype": "torch.float32"}
            
            # Determine target dtype
            original_dtype_str = metadata.get("dtype", "torch.float32")
            if "bfloat16" in original_dtype_str:
                target_dtype = torch.bfloat16
            elif "float16" in original_dtype_str:
                target_dtype = torch.float16
            else:
                target_dtype = torch.float32
            
            # Find all embedding files
            import glob
            embedding_files = sorted(glob.glob(os.path.join(doc_dir, "embedding_*.npy")))
            
            if not embedding_files:
                return None
                
            # Load each embedding
            embeddings = []
            for emb_file in embedding_files:
                embedding_np = np.load(emb_file)
                # Create as float32 then convert if needed
                embedding = torch.tensor(embedding_np, dtype=torch.float32)
                if target_dtype != torch.float32:
                    embedding = embedding.to(target_dtype)
                embeddings.append(embedding)
            
            print(f"Successfully loaded {len(embeddings)} embeddings for {file_name}")
            return embeddings
        except Exception as e:
            print(f"Error in direct load: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def load_embeddings(self, file_path: str) -> Optional[List[torch.Tensor]]:
        """Load document embeddings from the database."""
        # Use direct file loading as a more reliable method
        return self.load_embeddings_direct(file_path)
    
    def delete_embeddings(self, file_path: str) -> bool:
        """Delete embeddings for a file from the database."""
        try:
            table_name = self.get_table_name_for_file(file_path)
            doc_dir = os.path.join(self.db_path, "embeddings", table_name)
            
            if os.path.exists(doc_dir):
                import shutil
                shutil.rmtree(doc_dir)
                print(f"Deleted embeddings for {os.path.basename(file_path)}")
                return True
                
            # Also try to delete from LanceDB if it exists
            if table_name in self.db.table_names():
                self.db.drop_table(table_name)
                return True
                
            return False
        except Exception as e:
            print(f"Error deleting embeddings: {e}")
            return False
            
    def list_documents(self) -> List[Dict[str, Any]]:
        """List all documents with embeddings in the database."""
        try:
            documents = []
            
            # Check direct storage
            embeddings_dir = os.path.join(self.db_path, "embeddings")
            if os.path.exists(embeddings_dir):
                import os
                for table_name in os.listdir(embeddings_dir):
                    doc_dir = os.path.join(embeddings_dir, table_name)
                    if os.path.isdir(doc_dir):
                        # Load metadata
                        import json
                        try:
                            with open(os.path.join(doc_dir, "metadata.json"), 'r') as f:
                                metadata = json.load(f)
                                metadata["table_name"] = table_name
                                documents.append(metadata)
                        except:
                            # If metadata file is missing, create a basic entry
                            documents.append({
                                "filename": table_name.replace("doc_", ""),
                                "table_name": table_name,
                                "page_count": 0,
                                "embedding_dimension": 0
                            })
            
            return documents
        except Exception as e:
            print(f"Error listing documents: {e}")
            return []

================
File: docker-compose.yml
================
version: '3.8'

services:
  colpali-app:
    build: .
    ports:
      - "7860:7860"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - API_KEYS=${API_KEYS}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics
      - CUDA_VISIBLE_DEVICES=0
    runtime: nvidia  # This works with older Docker versions
    deploy:  # This works with newer Docker versions
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, utility, compute]
    restart: unless-stopped

================
File: docker-entrypoint.sh
================
#!/bin/bash
set -e

echo "=== Initializing Colpali Model Environment ==="

# Create necessary directories with explicit structure
mkdir -p /app/data/embeddings_db
mkdir -p /app/models/colqwen2/model
mkdir -p /app/models/colqwen2/processor

# Check if directories exist and are properly mounted
if [ ! -w "/app/models/colqwen2/model" ]; then
    echo "WARNING: Cannot write to model directory. Volume may not be properly mounted!"
else
    echo "âœ… Model directory is properly mounted and writable"
    
    # Create a test file to verify write permissions
    echo "Testing write permissions..." > /app/models/colqwen2/model/test_write.tmp
    rm /app/models/colqwen2/model/test_write.tmp
fi

# Set proper permissions
chmod -R 777 /app/data
chmod -R 777 /app/models

# List existing model files
echo "=== Model Directory Contents ==="
ls -la /app/models/colqwen2/model || echo "No model files yet"
ls -la /app/models/colqwen2/processor || echo "No processor files yet"

# Execute the provided command (usually python app.py)
echo "=== Starting Application ==="
exec "$@"

================
File: Dockerfile
================
FROM nvidia/cuda:12.6.0-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV NVIDIA_VISIBLE_DEVICES all
ENV NVIDIA_DRIVER_CAPABILITIES compute,utility
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    poppler-utils \
    libgl1-mesa-glx \
    libglib2.0-0 \
    build-essential \
    wget \
    git \
    && ln -sf /usr/bin/python3 /usr/bin/python \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies
# First ensure pip is up to date
RUN pip install --upgrade pip setuptools wheel

# Copy requirements file first for better caching
COPY requirements.txt .

# Create a modified requirements file without PyTorch packages
RUN grep -v "torch\|torchaudio\|torchvision" requirements.txt > requirements_filtered.txt

# Install PyTorch with CUDA 12.6 support
# Using version 2.7.0+cu126 which is available with CUDA 12.6
RUN pip install torch==2.7.0+cu126 torchvision==0.22.0+cu126 torchaudio==2.7.0+cu126 --index-url https://download.pytorch.org/whl/cu126

# Install other dependencies
RUN pip install -r requirements_filtered.txt

# Create directories for storage and model files
RUN mkdir -p ./data/embeddings_db ./models/colqwen2/model ./models/colqwen2/processor

# Create config.json files in the container directly instead of copying them
RUN echo '{\n\
  "model_type": "colqwen2",\n\
  "architectures": ["ColQwen2ForVisionText"],\n\
  "_name_or_path": "vidore/colqwen2-v1.0",\n\
  "hidden_size": 2048,\n\
  "num_hidden_layers": 24,\n\
  "num_attention_heads": 16,\n\
  "intermediate_size": 8192,\n\
  "hidden_act": "silu",\n\
  "max_position_embeddings": 4096,\n\
  "use_cache": true,\n\
  "bos_token_id": 1,\n\
  "eos_token_id": 2,\n\
  "rope_theta": 10000.0,\n\
  "use_flash_attn": false,\n\
  "tie_word_embeddings": false,\n\
  "pad_token_id": 0,\n\
  "vocab_size": 151936,\n\
  "attention_dropout": 0.0,\n\
  "initializer_range": 0.02,\n\
  "layernorm_epsilon": 1e-5,\n\
  "rms_norm_eps": 1e-6,\n\
  "transformers_version": "4.47.1",\n\
  "torch_dtype": "bfloat16"\n\
}' > ./models/colqwen2/model/config.json

RUN echo '{\n\
  "processor_class": "ColQwen2Processor",\n\
  "is_vision_text_model": true,\n\
  "image_size": 224,\n\
  "image_mean": [0.48145466, 0.4578275, 0.40821073],\n\
  "image_std": [0.26862954, 0.26130258, 0.27577711],\n\
  "feature_extractor_type": "ColQwen2FeatureExtractor",\n\
  "tokenizer_class": "PreTrainedTokenizerFast",\n\
  "model_max_length": 4096,\n\
  "padding_side": "right",\n\
  "truncation_side": "right",\n\
  "pad_token_id": 0,\n\
  "eos_token_id": 2,\n\
  "bos_token_id": 1,\n\
  "sep_token_id": 3,\n\
  "pad_token": "<|endoftext|>",\n\
  "eos_token": "<|im_end|>",\n\
  "bos_token": "<|im_start|>",\n\
  "do_normalize": true,\n\
  "do_resize": true,\n\
  "do_center_crop": true,\n\
  "vision_text_processor": true,\n\
  "transformers_version": "4.47.1"\n\
}' > ./models/colqwen2/processor/config.json

# Copy application files
COPY . .

# Expose port
EXPOSE 7860

# Copy verification script and startup script
COPY verify_cuda.py .
COPY start-app.sh .
COPY docker-entrypoint.sh .
RUN chmod +x start-app.sh docker-entrypoint.sh

# Don't try to install flash-attn at build time - it's too slow and often fails
# Instead, we'll let the app try to install it at runtime if needed
RUN apt-get update && \
    apt-get install -y ninja-build && \
    rm -rf /var/lib/apt/lists/*

# Make sure we use our entrypoint script
ENTRYPOINT ["./docker-entrypoint.sh"]

# Command to run the application
CMD ["./start-app.sh"]

================
File: model_setup.sh
================
#!/bin/bash
# Script to set up model directory structure for ColPali
# Run this before starting the Docker container to ensure model persistence

echo "=== Setting up model directories for ColPali ==="

# Create full directory structure
mkdir -p ./models/colqwen2/model
mkdir -p ./models/colqwen2/processor
mkdir -p ./data/embeddings_db

# Set permissions for Docker container access
chmod -R 777 ./models
chmod -R 777 ./data

echo "âœ… Model directories created successfully"
echo "Directory structure:"
find ./models -type d | sort

echo ""
echo "You can now start the Docker container with:"
echo "docker-compose up -d"

================
File: packages.txt
================
poppler-utils

================
File: README.md
================
---
title: ColPali
emoji: ðŸƒ
colorFrom: pink
colorTo: green
sdk: gradio
sdk_version: 4.37.2
app_file: app.py
pinned: false
short_description: Document Retrieval
---

Check out the configuration reference at https://huggingface.co/docs/hub/spaces-config-reference

================
File: start-app.sh
================
#!/bin/bash
# Startup script for the ColPali Docker application

# Check environment
echo "=== System Information ==="
uname -a
nvidia-smi || echo "NVIDIA drivers not found or not accessible"

# Verify CUDA
echo "=== Running CUDA Verification ==="
python verify_cuda.py

# Create model directories on host if they don't exist
echo "=== Ensuring Model Directories Exist ==="
mkdir -p ./models/colqwen2/model
mkdir -p ./models/colqwen2/processor
chmod -R 777 ./models

# List current model files
echo "=== Current Model Files ==="
ls -la ./models/colqwen2/model || echo "Model directory is empty"
ls -la ./models/colqwen2/processor || echo "Processor directory is empty"

# Handle flash-attn installation
echo "=== Checking Flash Attention Requirement ==="
if grep -q "install_fa2" app.py; then
  echo "Flash Attention is used in the app, but we'll let the app handle its installation if needed"
  echo "If the app tries to install flash-attn automatically, it might take several minutes"
  echo "You can check app.py if it actually needs flash-attn or if it can be disabled"
fi

# Verify torch version
echo "=== PyTorch Configuration ==="
python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda if torch.cuda.is_available() else \"N/A\"}')"

# Create necessary directories
mkdir -p ./data/embeddings_db

# Launch application
echo "=== Starting ColPali Application ==="
python app.py

================
File: verify_cuda.py
================
"""
CUDA verification script - run this to check if CUDA is properly configured
"""

import torch
import os

print("=== CUDA Environment Verification ===")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"CUDA device count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"CUDA device {i} name: {torch.cuda.get_device_name(i)}")
    print(f"Current CUDA device: {torch.cuda.current_device()}")
else:
    print("CUDA is not available. GPU acceleration will not be used.")

print("\n=== Environment Variables ===")
print(f"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not set')}")
print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}")
print(f"NVIDIA_VISIBLE_DEVICES: {os.environ.get('NVIDIA_VISIBLE_DEVICES', 'Not set')}")

# Try a simple CUDA operation
if torch.cuda.is_available():
    print("\n=== Testing CUDA Operation ===")
    try:
        x = torch.rand(5, 3).cuda()
        y = torch.rand(5, 3).cuda()
        z = x + y
        print("CUDA operation successful!")
    except Exception as e:
        print(f"CUDA operation failed: {e}")
